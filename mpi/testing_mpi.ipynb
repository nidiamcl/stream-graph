{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix, spmatrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from scipy import spatial\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse\n",
    "import scipy as sp\n",
    "import tqdm as tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotSimilarity(fp, vec):\n",
    "    ''' gets similarity between a fingerprint and a row vector\n",
    "        the number of non-zero components they share \n",
    "        divided by the total number of non-zero components of the vector '''\n",
    "    return vec.dot(fp).max() / vec.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(fp,vec):\n",
    "    if scipy.sparse.isspmatrix_csr(vec):\n",
    "        return 1 - spatial.distance.cosine(fp, vec.A.astype(np.float))\n",
    "    else:\n",
    "        return 1 - spatial.distance.cosine(fp, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMISimilarity(fp,vec):\n",
    "    if scipy.sparse.isspmatrix_csr(vec):       \n",
    "        return normalized_mutual_info_score(fp,vec.A.astype(np.float).flatten())\n",
    "    else:\n",
    "        return normalized_mutual_info_score(fp, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFingerprint(fp, vec, count):\n",
    "    ''' updates a fingerprint when a node vector is added to the cluster\n",
    "        weighted merge of the node vector with the fingerprint '''\n",
    "    if scipy.sparse.isspmatrix_csr(vec):\n",
    "        return (fp * ((count-1)/count)) + (vec.A.astype(np.float) * (1/count))\n",
    "    else:\n",
    "        return (fp * ((count-1)/count)) + (vec*(1/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClusters(nodes, csr_matrix, similarity='dotsim', threshold=0.5):\n",
    "    ''' fingerprint map\n",
    "    {\n",
    "        fp_index: [\n",
    "            row_index,\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    }\n",
    "    '''\n",
    "    # mapping of nodes to fingerprints to keep track of what node belongs to what fp\n",
    "    fmap = defaultdict(list)\n",
    "\n",
    "    ''' fingerprints '''\n",
    "    fps = []\n",
    "\n",
    "    for ri, node in enumerate(nodes):\n",
    "#         print(ri,node)\n",
    "        row = csr_matrix[ri]\n",
    "        \n",
    "        # initialize fingerprints\n",
    "        if len(fps) == 0:\n",
    "            fmap[len(fps)].append(node)\n",
    "            fps.append(row.A[0].astype(np.float))\n",
    "            continue\n",
    "        \n",
    "        # get best scoring fingerprint using dotSimilarity\n",
    "        if similarity == 'dotsim':\n",
    "            print('initial similarity metric: dot_similarity')\n",
    "            \n",
    "            # sorted and pop gets me the best scoring one (find something more elegant!)\n",
    "            score, fi, fp = sorted([(dotSimilarity(fp, row), fi, fp) for fi, fp in enumerate(fps)]).pop() \n",
    "\n",
    "        # get best scoring fingerprint using cosine Similarity\n",
    "        elif similarity == 'cosine':\n",
    "            print('initial similarity metric: cosine_similarity')\n",
    "            score, fi, fp = sorted([(cosineSimilarity(fp, row), fi, fp) for fi, fp in enumerate(fps)]).pop() \n",
    "        \n",
    "        # get best scoring fingerprint using mutual_info_score\n",
    "        elif similarity == 'nmi':\n",
    "            print('initial similarity metric: normalized_mutual_information')\n",
    "            score, fi, fp = sorted([(NMISimilarity(fp, row), fi, fp) for fi, fp in enumerate(fps)]).pop()  \n",
    "\n",
    "        if score > threshold:\n",
    "            print(score, threshold)\n",
    "            \n",
    "            # map node to fingerprint\n",
    "            fmap[fi].append(node)\n",
    "            # update fingerprint with row weights\n",
    "            fp[:] = updateFingerprint(fp, row, len(fmap[fi]))\n",
    "            \n",
    "        else:\n",
    "            fmap[len(fps)].append(node)\n",
    "            fps.append(row.A[0].astype(np.float))\n",
    "            \n",
    "    return fps, fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeFingerprints(fps, fmap, similarity='dotsim', threshold=0.3):\n",
    "    '''\n",
    "    finds similar clusters and merges them  \n",
    "    '''\n",
    "    # same kind of mapping of nodes to fingerprints\n",
    "    merged_fps = []\n",
    "    merged_fmap = {}\n",
    "\n",
    "    processed = []\n",
    "    for ai, afp in enumerate(fps):\n",
    "        # skip fingerprints that have already been merged\n",
    "        if ai in processed: continue\n",
    "            \n",
    "        # dot similarity\n",
    "        if similarity == 'dotsim':\n",
    "            print('merging similarity metric: dot_similarity')\n",
    "            score, bi, bfp = sorted([(dotSimilarity(afp, bfp), bi, bfp) for bi, bfp in enumerate(fps) if bi != ai]).pop()\n",
    "\n",
    "        # normalized mutual info score\n",
    "        elif similarity == 'nmi':\n",
    "            print('initial similarity metric: normalized_mutual_information')\n",
    "            score, bi, bfp = sorted([(NMISimilarity(afp, bfp), bi, bfp) for bi, bfp in enumerate(fps) if bi != ai]).pop()\n",
    "\n",
    "        \n",
    "        # cosine Similarity\n",
    "        elif similarity == 'cosine':\n",
    "            print('initial similarity metric: cosine_similarity')\n",
    "            score, bi, bfp = sorted([(cosineSimilarity(afp, bfp), bi, bfp) for bi, bfp in enumerate(fps) if bi != ai]).pop() \n",
    "                      \n",
    "        # same for second fingerprint\n",
    "        if bi in processed: continue\n",
    "\n",
    "        if score > threshold:\n",
    "            # merge fingerprints\n",
    "            fp = updateFingerprint(afp, bfp, 2)\n",
    "            merged_fps.append(fp)\n",
    "            # merge node references\n",
    "            i = len(merged_fps) - 1\n",
    "            merged_fmap[i] = list(set(fmap[ai] + fmap[bi]))\n",
    "            # mark as processed\n",
    "            processed += [ai, bi]\n",
    "            \n",
    "    # add fingerprints that were not merged\n",
    "    for i, fp in enumerate(fps):\n",
    "        if i not in processed:\n",
    "            merged_fps.append(fp)\n",
    "            merged_fmap[len(merged_fps) - 1] = fmap[i]\n",
    "\n",
    "    return merged_fps, merged_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mergeFingerprints(fps, fmap, threshold=0.3):\n",
    "#     '''\n",
    "#     finds similar clusters and merges them  \n",
    "#     '''\n",
    "#     # same kind of mapping of nodes to fingerprints\n",
    "#     merged_fps = []\n",
    "#     merged_fmap = {}\n",
    "\n",
    "#     processed = []\n",
    "#     for ai, afp in enumerate(fps):\n",
    "#         # skip fingerprints that have already been merged\n",
    "#         if ai in processed: continue\n",
    "\n",
    "#         # get best scoring fingerprint using dotSimilarity\n",
    "# #         score, bi, bfp = sorted([(dotSimilarity(afp, bfp), bi, bfp) for bi, bfp in enumerate(fps) if bi != ai]).pop()\n",
    "\n",
    "#         # get best scoring fingerprint using mutual_info_score\n",
    "#         # normalized_mutual_info_score(x,y) \n",
    "#         # adjusted_mutual_info_score(x,y)\n",
    "#         score, bi, bfp = sorted([(normalized_mutual_info_score(afp, bfp), bi, bfp) for bi, bfp in enumerate(fps) if bi != ai]).pop()\n",
    "\n",
    "#         # same for second fingerprint\n",
    "#         if bi in processed: continue\n",
    "\n",
    "#         if score > threshold:\n",
    "#             # merge fingerprints\n",
    "#             fp = updateFingerprint(afp, bfp, 2)\n",
    "#             merged_fps.append(fp)\n",
    "#             # merge node references\n",
    "#             i = len(merged_fps) - 1\n",
    "#             merged_fmap[i] = list(set(fmap[ai] + fmap[bi]))\n",
    "#             # mark as processed\n",
    "#             processed += [ai, bi]\n",
    "            \n",
    "#     # add fingerprints that were not merged\n",
    "#     for i, fp in enumerate(fps):\n",
    "#         if i not in processed:\n",
    "#             merged_fps.append(fp)\n",
    "#             merged_fmap[len(merged_fps) - 1] = fmap[i]\n",
    "\n",
    "#     return merged_fps, merged_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = np.load('../sample_data/trajectory.npy')\n",
    "t = np.matrix(trajectory)\n",
    "csr_trajectory = sparse.csr_matrix(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9048233444321127 0.9\n",
      "0.9009808866913572 0.9\n",
      "0.9006851586309836 0.9\n",
      "0.9006709689565728 0.9\n",
      "0.9006546599807018 0.9\n",
      "0.9006829834407262 0.9\n",
      "0.9006891956037538 0.9\n",
      "0.9006439176737459 0.9\n",
      "0.9009380642260487 0.9\n",
      "0.9032759360190811 0.9\n",
      "0.9003231613767171 0.9\n",
      "0.9027970822339201 0.9\n",
      "0.9007281186742803 0.9\n",
      "0.9010465327398122 0.9\n",
      "0.900194638638703 0.9\n",
      "clusters found: 9986\n"
     ]
    }
   ],
   "source": [
    "# choose thresholds\n",
    "first_th = 0.7\n",
    "second_th = 0.9\n",
    "\n",
    "# find initial clusters\n",
    "# findClusters returns fps (list) and fmap (dict)\n",
    "fps, fmap = findClusters(rows, t, first_th)\n",
    "print('clusters found: ' + str(len(fmap)))\n",
    "\n",
    "# merge similar clusters\n",
    "merged_fps, merged_fmap = mergeFingerprints(fps, fmap, second_th)\n",
    "print('clusters merged: ' + str(len(fmap)-len(merged_fmap)))\n",
    "print('remaining clusters: ' + str(len(merged_fmap)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING THIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes: 12\n"
     ]
    }
   ],
   "source": [
    "from reader import *\n",
    "from mpi4py import MPI\n",
    "\n",
    "file = '../sample_data/test.tsv'\n",
    "g = nx.read_edgelist(file, delimiter=' ', nodetype=int, edgetype=int, create_using=nx.Graph())\n",
    "nodes = g.nodes()\n",
    "\n",
    "gr = GraphReader('../sample_data/test.tsv', '../sample_data/test_node_edges.txt')\n",
    "csr_matrix = gr.read() # sparse matrix\n",
    "\n",
    "print('nodes: ' + str(len(g.nodes)))\n",
    "csr_A = nx.to_scipy_sparse_matrix(g, format='csr', dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x12 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12x12 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 50 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "          True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "          True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "          True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "          True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False,  True,  True, False,  True,  True,\n",
       "         False, False, False],\n",
       "        [ True,  True,  True,  True, False, False,  True, False, False,\n",
       "          True, False, False],\n",
       "        [False, False, False, False, False,  True, False,  True,  True,\n",
       "         False, False,  True],\n",
       "        [ True,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "          True, False,  True],\n",
       "        [ True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "          True, False, False],\n",
       "        [ True,  True,  True,  True,  True, False, False, False, False,\n",
       "         False,  True, False],\n",
       "        [ True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "         False, False,  True]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix.todense() == csr_A.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_A.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial similarity metric: normalized_mutual_information\n",
      "0.3407833215844391 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.5817448669547886 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.6051084728780068 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.4134132143817069 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.4000540705765895 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.5456278264547397 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.3500037108861601 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.391623268955809 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.391623268955809 0.222\n",
      "initial similarity metric: normalized_mutual_information\n",
      "0.6254165429705678 0.222\n",
      "clusters found: 2\n",
      "initial similarity metric: normalized_mutual_information\n",
      "initial similarity metric: normalized_mutual_information\n",
      "clusters merged: 0\n",
      "remaining clusters: 2\n"
     ]
    }
   ],
   "source": [
    "# choose thresholds\n",
    "first_th = 0.222\n",
    "second_th = 0.5\n",
    "\n",
    "# find initial clusters\n",
    "# findClusters returns fps (list) and fmap (dict)\n",
    "fps, fmap = findClusters(nodes, csr_matrix, similarity='nmi', threshold=first_th)\n",
    "print('clusters found: ' + str(len(fmap)))\n",
    "\n",
    "# merge similar clusters\n",
    "# merge similar clusters\n",
    "merged_fps, merged_fmap = mergeFingerprints(fps, fmap, similarity='nmi', threshold=second_th)\n",
    "print('clusters merged: ' + str(len(fmap)-len(merged_fmap)))\n",
    "print('remaining clusters: ' + str(len(merged_fmap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0]]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-9-efac57266dc8>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-efac57266dc8>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    fps, fmap = findClusters(nodes, csr_matrix, similarity='dotsim', first_th)\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# choose thresholds\n",
    "first_th = 0.222\n",
    "second_th = 0.05\n",
    "\n",
    "# find initial clusters\n",
    "# findClusters returns fps (list) and fmap (dict)\n",
    "fps, fmap = findClusters(nodes, csr_matrix, similarity='dotsim', first_th)\n",
    "print('clusters found: ' + str(len(fmap)))\n",
    "\n",
    "# merge similar clusters\n",
    "# merge similar clusters\n",
    "merged_fps, merged_fmap = mergeFingerprints(fps, fmap, second_th)\n",
    "print('clusters merged: ' + str(len(fmap)-len(merged_fmap)))\n",
    "print('remaining clusters: ' + str(len(merged_fmap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 27\n",
      "Number of edges: 111\n",
      "Average degree:   8.2222\n",
      "Density: 0.3162393162393162\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3a6b516f9d78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# find initial clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# findClusters returns fps (list) and fmap (dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindClusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_th\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clusters found: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/stream-graph/mpi/clusters.py\u001b[0m in \u001b[0;36mfindClusters\u001b[0;34m(nodes, csr_matrix, threshold)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#         print(ri,node)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mri\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# initialize fingerprints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# running this cell will load the functions from the clusters file \n",
    "from clusters import *\n",
    "\n",
    "# you need a networkx graph, this is a small one for testing\n",
    "file = '../sample_data/zebra.tsv'\n",
    "g = nx.read_edgelist(file, delimiter=' ', nodetype=int, create_using=nx.Graph())\n",
    "\n",
    "print(nx.info(g))\n",
    "print('Density: ' + str(nx.density(g)))\n",
    "\n",
    "# choose thresholds\n",
    "first_th = 0.222\n",
    "second_th = 0.05\n",
    "\n",
    "# find initial clusters\n",
    "# findClusters returns fps (list) and fmap (dict)\n",
    "fps, fmap = findClusters(g, first_th)\n",
    "print('clusters found: ' + str(len(fmap)))\n",
    "\n",
    "# merge similar clusters\n",
    "merged_fps, merged_fmap = mergeFingerprints(fps, fmap, second_th)\n",
    "print('clusters merged: ' + str(len(fmap)-len(merged_fmap)))\n",
    "print('remaining clusters: ' + str(len(merged_fmap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
